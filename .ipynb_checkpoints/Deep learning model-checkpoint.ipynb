{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c76a24b-8702-4d03-9f48-fa3463905a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import  confusion_matrix,multilabel_confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def  train_test_split(data):\n",
    "     data2=data.groupby(by=['classes']).sample(frac=0.05, random_state=42)\n",
    "     print(data2.shape) \n",
    "     x_data=data_image2[['r','g','b']]\n",
    "     y_data=data_image2['classes2']\n",
    "     print(y_data.unique())\n",
    "     X_train, X_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.1, random_state=42)\n",
    "     print(y_train.unique())\n",
    "     print(y_test.unique())\n",
    "     y_train_encoded = to_categorical(y_train,num_classes=5)\n",
    "     y_test_encoded = to_categorical(y_test,num_classes=5) \n",
    "     return X_train, X_test,  y_train_encoded,y_test_encoded\n",
    "\n",
    "\n",
    "def model_predict(model,X,Y,labels):\n",
    "    # predict\n",
    "    y_pred = model.predict(X)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)  # Convert probabilities to class labels\n",
    "\n",
    "    # Calculate classification report\n",
    "    classification_report1 = classification_report(Y, y_pred_classes,labels=labels,output_dict=True)\n",
    "    # Calculate confusion_matrix\n",
    "    multi_confusion_matrix1=multilabel_confusion_matrix(Y, y_pred_classes,labels=labels)\n",
    "    confusion_matrix1= confusion_matrix(Y, y_pred_classes,labels=labels)\n",
    "    \n",
    "    return  classification_report1,  confusion_matrix1,multi_confusion_matrix1\n",
    " \n",
    "   \n",
    "def plot_model_results(model_history_df):\n",
    "    \n",
    "    acc =model_history_df['accuracy']\n",
    "    val_acc = model_history_df['val_accuracy']\n",
    "    loss =model_history_df['loss']\n",
    "    val_loss =model_history_df['val_loss']\n",
    "\n",
    "    epochs_range = range(model_history_df.shape[0])\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "    plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs_range, loss, label='Training Loss')\n",
    "    plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "def model_summary(classification_report,multi_confusion_matrix,confusion_matrix,classes):\n",
    "    \n",
    "    # classification_report to DataFrame\n",
    "    #required_fields=['Black Soil', 'Cinder Soil', 'Laterite Soil', 'Peat Soil', 'Yellow Soil','macro avg', 'weighted avg']#, 'weighted avg'\n",
    "    required_fields=['0', '1', '2', '3', '4', 'macro avg', 'weighted avg']\n",
    "    classification_report_dict={k:v['precision'] for k,v in classification_report.items() if k in required_fields}\n",
    "    classification_report_dict2={k:v['support'] for k,v in classification_report.items() if k in required_fields}\n",
    "    classification_report_dict.update({'accuracy':classification_report1['accuracy']})\n",
    "    classification_report_df=pd.DataFrame(classification_report_dict,index=['precision']).T\n",
    "    classification_report_df.index=['Black Soil', 'Cinder Soil', 'Laterite Soil', 'Peat Soil', 'Yellow Soil','macro avg', 'weighted avg','accuracy']\n",
    "    classification_report_df= classification_report_df.sort_values(by = 'precision', ascending=False)#\n",
    "    \n",
    "    # confusion_matrix to  DataFrame\n",
    "    classes=['Black Soil', 'Cinder Soil', 'Laterite Soil', 'Peat Soil', 'Yellow Soil']\n",
    "    dict_matrix=dict()\n",
    "    for i,l in zip(range(multi_confusion_matrix.shape[0]),classes):\n",
    "        cm=multi_confusion_matrix1[i,:,:]\n",
    "        df_cm = pd.DataFrame(cm,index= ['other',l], columns=['other',l])\n",
    "        #confusion_matrix, index=class_names, columns=class_names,\n",
    "        dict_matrix.update({l:df_cm})\n",
    "    \n",
    "    classes=['Black Soil', 'Cinder Soil', 'Laterite Soil', 'Peat Soil', 'Yellow Soil']\n",
    "    confusion_matrix_df= pd.DataFrame(confusion_matrix,index=classes, columns=classes)\n",
    "    return classification_report_df, dict_matrix, confusion_matrix_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define a function that returns your TensorFlow model\n",
    "def create_FCNNsmodel():\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(3,)))\n",
    "    model.add(Dense(16, activation='tanh'))\n",
    "    model.add(Dropout(rate=0.2))\n",
    "    model.add(Dense(16, activation='tanh'))\n",
    "    model.add(Dense(16, activation='tanh'))\n",
    "    model.add(Dense(5, activation='softmax'))\n",
    "   \n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def create_CNNmodel():                      \n",
    "    model = Sequential([\n",
    "    layers.Rescaling(1./255, input_shape=(img_height, img_width, 3)),\n",
    "    layers.Conv2D(16, 3, padding='same', activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(5,activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "   \n",
    "\n",
    "    \n",
    "def Keras_Classifier(build_fn=create_FCNNsmodel):\n",
    "    # Wrap the TensorFlow model using KerasClassifier\n",
    "    keras_model = KerasClassifier(build_fn=build_fn)\n",
    "    # Define the hyperparameter grid for the grid search\n",
    "    param_grid = {\n",
    "    'batch_size': [16, 32],\n",
    "    'epochs': [10, 20, 30]\n",
    "     }\n",
    "\n",
    "   # Create an instance of the EarlyStopping callback\n",
    "    early_stopping_monitor = EarlyStopping(\n",
    "    monitor='val_loss',  # Metric to monitor (e.g., validation loss)\n",
    "    patience=3,  # Number of epochs to wait for improvement\n",
    "    restore_best_weights=True  # Restores the weights of the best epoch\n",
    "  )\n",
    "    # Create a dictionary of callbacks\n",
    "    #callbacks = {\n",
    "    #'early_stopping': early_stopping_monitor\n",
    "   #}\n",
    "\n",
    "    # Perform grid search with cross-validation and include validation data and callbacks\n",
    "    grid_search = GridSearchCV(estimator=keras_model, param_grid=param_grid, cv=3) \n",
    "    return grid_search \n",
    "     \n",
    "    \n",
    "    \n",
    "     \n",
    "\n",
    "#model.summary()\n",
    "\n",
    "#model.compile(optimizer='adam',\n",
    "              #loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "             # metrics=['accuracy'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81037327-c2b6-4879-bfef-cb6eae54cf9c",
   "metadata": {},
   "source": [
    "# get the data df from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f71904-9609-4a1f-8be8-70ac463fbca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Soil_types_directory= 'C:/Users/user/Google Drive/TCD20/python/dataset/Soil types'\n",
    "data_image=pd.read_csv(os.path.join(Soil_types_directory,'all_image_32.csv'))\n",
    "data_image_mean=pd.read_csv(os.path.join(Soil_types_directory,'all_image_mean.csv'))\n",
    "data_image_select_image=pd.read_csv(os.path.join(Soil_types_directory,'Selecting_image_64.csv'))\n",
    "data_image_cleaning=pd.read_csv(os.path.join(Soil_types_directory,'all_image_cleaning_128.csv'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf74cfe4-9800-4f90-bf09-8bf401c66474",
   "metadata": {},
   "source": [
    "### train test split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2b5742-4337-4232-b027-34ca1eb4853f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "#data_image2=data_image.groupby(by=['classes']).sample(frac=0.05, random_state=42)\n",
    "#print(data_image2.shape)\n",
    "\n",
    "\n",
    "#x_data=data_image2[['r','g','b']]\n",
    "#y_data=data_image2['classes2']\n",
    "#print(y_data.unique())\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.1, random_state=42)\n",
    "#print(y_train.unique())\n",
    "#print(y_test.unique())\n",
    "\n",
    "\n",
    "\n",
    "#y_train_encoded = to_categorical(y_train,num_classes=5)\n",
    "#y_test_encoded = to_categorical(y_test,num_classes=5)\n",
    "#y_test_encoded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8c2c65-38b0-49f8-8320-14ea47d9b6da",
   "metadata": {},
   "source": [
    "# from df   fully connected network (FNNs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6160a85-52c6-48c8-9787-cc1f51dcc980",
   "metadata": {},
   "source": [
    "\n",
    "### Set up the layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66ff4c2-fe8b-4db5-a385-5f7e4efdc8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "# crater a model \n",
    "model=create_FCNNsmodel()\n",
    "model.summary()\n",
    "optimizer = Adam(learning_rate=0.0001)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "#tf.keras.utils.plot_model(model, show_dtype=True, show_shapes=True, show_layer_names=True, to_file='model_1.png')\n",
    "\n",
    "\n",
    "\n",
    "early_stopping_monitor = EarlyStopping(\n",
    "    monitor='accuracy',\n",
    "    min_delta=0,\n",
    "    patience=5,\n",
    "    verbose=0,\n",
    "    mode='auto',\n",
    "    baseline=None,\n",
    "    restore_best_weights=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0170e692-b5d2-405e-9dfc-8f58a516a7ef",
   "metadata": {},
   "source": [
    "## data image all pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc15c5af-6c4d-4700-9b85-b1a114df2dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=20\n",
    "X_train,,X_test ,y_train_encoded,y_test_encoded= train_test_split(data_image)\n",
    "\n",
    "history=model.fit(X_train, y_train_encoded,\n",
    "          validation_data=(X_test,y_test_encoded ),\n",
    "          batch_size=16, epochs=epochs, callbacks=[early_stopping_monitor] \n",
    "          , verbose=True)\n",
    "\n",
    "history_df=pd.DataFrame(history.history)\n",
    "history_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d19dc34-a2df-45c0-b282-ae6f2d6eeb0f",
   "metadata": {},
   "source": [
    "### Summary model results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561f7e56-03fb-4b0c-a1fe-f8a48f8eea7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "labels=['Black Soil', 'Cinder Soil', 'Laterite Soil', 'Peat Soil', 'Yellow Soil']\n",
    "labels=[0,1,2,3,4]\n",
    "\n",
    "classification_report1,confusion_matrix1,multi_confusion_matrix1=model_predict(model,X_test, y_test,labels)\n",
    "classification_report_df, dict_matrix, confusion_matrix_df=model_summary(classification_report1,multi_confusion_matrix1,confusion_matrix1,labels)\n",
    "plot_model_results(history_df,epochs)\n",
    "\n",
    "\n",
    "\n",
    "#classification_report_df\n",
    "print('### Yellow Soil ###')\n",
    "print(dict_matrix['Yellow Soil'])\n",
    "print('### Laterite Soil\t ###')\n",
    "print(dict_matrix['Laterite Soil'])\n",
    "print('### Black Soil ###')\n",
    "print(dict_matrix['Black Soil'])\n",
    "print('### Cinder Soill\t ###')\n",
    "print(dict_matrix['Cinder Soil'])\n",
    "print('### Peat Soil\t ###')\n",
    "print(dict_matrix['Peat Soil'])\n",
    "\n",
    "\n",
    "print(confusion_matrix_df)\n",
    "classification_report_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214b63f4-b18e-4701-8b87-7b1a0e9b1adf",
   "metadata": {},
   "source": [
    "### optimization of batch and epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035a97b3-6a5b-4536-9f7a-c58200965c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Wrap the TensorFlow model using KerasClassifier\n",
    "keras_model = KerasClassifier(build_fn=create_FCNNsmodel)\n",
    "\n",
    "# Define the hyperparameter grid for the grid search\n",
    "param_grid = {\n",
    "    'batch_size': [16, 32],\n",
    "    'epochs': [10, 20, 30]\n",
    "}\n",
    "\n",
    "\n",
    "# Create an instance of the EarlyStopping callback\n",
    "early_stopping_monitor = EarlyStopping(\n",
    "    monitor='val_loss',  # Metric to monitor (e.g., validation loss)\n",
    "    patience=3,  # Number of epochs to wait for improvement\n",
    "    restore_best_weights=True  # Restores the weights of the best epoch\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Create a dictionary of callbacks\n",
    "callbacks = {\n",
    "    'early_stopping': early_stopping_monitor\n",
    "}\n",
    "\n",
    "# Perform grid search with cross-validation and include validation data and callbacks\n",
    "grid_search = GridSearchCV(estimator=keras_model, param_grid=param_grid, cv=3)\n",
    "\n",
    "\n",
    "# from def \n",
    "grid_search2=Keras_Classifier(build_fn=create_FCNNsmodel)\n",
    "\n",
    "#############################################################################################################\n",
    "grid_search.fit(X_train, y_train_encoded, validation_data=(X_test,y_test_encoded), callbacks=callbacks)\n",
    "\n",
    "# Print the best parameters and score\n",
    "print(\"Best parameters: \", grid_search.best_params_)\n",
    "print(\"Best score: \", grid_search.best_score_)\n",
    "\n",
    "# Access the training and validation scores for each parameter combination\n",
    "cv_results = grid_search.cv_results_\n",
    "train_scores = cv_results['mean_train_score']\n",
    "val_scores = cv_results['mean_test_score']\n",
    "\n",
    "# Print the training and validation scores for each parameter combination\n",
    "for train_score, val_score, params in zip(train_scores, val_scores, cv_results['params']):\n",
    "    print(f\"Train Score: {train_score:.4f}, Validation Score: {val_score:.4f}, Params: {params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93c96b7-91bf-45a4-af34-f2df1e69e92b",
   "metadata": {},
   "source": [
    "### Summary model results ?????????????????????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be38471-80d9-43bd-8695-0bc63ea87bba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551be9e8-f3c1-4dad-b8d8-cb4be5b0cc70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f7c36533-423c-43bf-8b86-062e65962a7c",
   "metadata": {},
   "source": [
    "## data image Selecting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e01e9da-9d83-4656-b31a-214e022d1039",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5dc5ec-7b19-4fac-ba2c-735fa24a8326",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4f2792ca-0209-405b-b8e3-b3cc35f1b600",
   "metadata": {},
   "source": [
    "## data image  cleaning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444b0255-58e5-4d80-937c-72e267ebe04b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25420272-28f2-44cb-a514-0d730297fef8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9f5c66-f9a3-4a8e-b5d5-5e811605aa56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9804b5e4-e251-49a1-b08b-d71d2eeb807f",
   "metadata": {},
   "source": [
    "# get the image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c540d7fc-4582-4195-9db0-131ff4237eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "data_dir= 'C:/Users/user/Google Drive/TCD20/python/dataset/Soil types'\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "img_height = 180\n",
    "img_width = 180\n",
    "\n",
    "train_ds= tf.keras.utils.image_dataset_from_directory(\n",
    "  data_dir,\n",
    "  validation_split=0.2,\n",
    "  subset=\"training\",\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)\n",
    "\n",
    "\n",
    "\n",
    "class_names =train_ds.class_names\n",
    "print(class_names)\n",
    "\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  data_dir,\n",
    "  validation_split=0.2,\n",
    "  subset=\"validation\",\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)\n",
    "\n",
    "\n",
    "# Create an instance of ImageDataGenerator with desired augmentation options\n",
    "data_augmentation = ImageDataGenerator(\n",
    "    rotation_range=10,  # Random rotation between -10 and +10 degrees\n",
    "    width_shift_range=0.1,  # Randomly shift the width by 10%\n",
    "    height_shift_range=0.1,  # Randomly shift the height by 10%\n",
    "    zoom_range=0.1,  # Randomly zoom by 10%\n",
    "    horizontal_flip=True  # Randomly flip horizontally\n",
    ")\n",
    "\n",
    "# Generate augmented data from the original dataset\n",
    "augmented_data = data_augmentation.flow_from_directory(\n",
    "    data_dir,  # Path to the directory containing the original images\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',  # Set the appropriate class mode\n",
    "    shuffle=True)  # Shuffle the data\n",
    " \n",
    "    \n",
    "y_train= []\n",
    "for images, labels in train_ds:\n",
    "    y_train.extend(labels.numpy())\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "\n",
    "y_test = []\n",
    "for images, labels in val_ds:\n",
    "    y_test.extend(labels.numpy())\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d13617-1c49-4621-91ef-dd25aba866bd",
   "metadata": {},
   "source": [
    "# from  image   Convolutional Neural Networks (CNNs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad2e44a-045c-407c-924b-493d392ec2c8",
   "metadata": {},
   "source": [
    "## Build the model\n",
    "### Set up the layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e6200d-68bd-4433-b31b-5f51154627cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy,SparseCategoricalCrossentropy\n",
    "\n",
    "num_classes = len(class_names)\n",
    "model = Sequential([\n",
    "  layers.Rescaling(1./255, input_shape=(img_height, img_width, 3)),\n",
    "  layers.Conv2D(16, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Flatten(),\n",
    "  layers.Dense(128, activation='relu'),\n",
    "  layers.Dense(num_classes,activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55f7f33-02dc-46f7-bb57-d13675f9f4aa",
   "metadata": {},
   "source": [
    "## Compile ane fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24144bf2-f0e4-41fb-b576-89869828f155",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_monitor = EarlyStopping(\n",
    "    monitor='val_loss',  # Metric to monitor (e.g., validation loss)\n",
    "    patience=3,  # Number of epochs to wait for improvement\n",
    "    restore_best_weights=True  # Restores the weights of the best epoch\n",
    ")\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "epochs=20\n",
    "history = model.fit(\n",
    "  train_ds,\n",
    "  validation_data=val_ds,\n",
    "  epochs=epochs,\n",
    "  callbacks=[early_stopping_monitor]  # Include the EarlyStopping callback  \n",
    ")\n",
    "\n",
    "history_df=pd.DataFrame(history.history)\n",
    "history_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38250c3f-6845-4471-9ae5-0ea6c2947757",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ec04257b-b206-4f60-a2aa-71e01d00ea1b",
   "metadata": {},
   "source": [
    "### Summary model results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c0a179-efaf-4656-9c71-2839448dc354",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=[0,1,2,3,4]\n",
    "classification_report1,confusion_matrix1,multi_confusion_matrix1=model_predict(model,train_ds, y_train,labels)\n",
    "classification_report_df, dict_matrix, confusion_matrix_df=model_summary(classification_report1,multi_confusion_matrix1,confusion_matrix1,labels)\n",
    "plot_model_results(history_df)\n",
    "\n",
    "\n",
    "print(classification_report_df)\n",
    "print(confusion_matrix_df)\n",
    "print(dict_matrix['Yellow Soil'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67e3a5f-22d8-40f4-9383-9701a6c9a30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential(\n",
    "  [\n",
    "    layers.RandomFlip(\"horizontal_and_vertical\",\n",
    "                      input_shape=(img_height,\n",
    "                                  img_width,\n",
    "                                  3)),\n",
    "    #tf.keras.layers.RandomCrop(\n",
    "    #35,5, seed=12),\n",
    "\n",
    "    layers.RandomRotation(0.1),\n",
    "    layers.RandomZoom(0.1),\n",
    "  ]\n",
    ")\n",
    "\n",
    "#data_augmentation = tf.keras.Sequential([\n",
    "  #layers.RandomFlip(\"horizontal_and_vertical\"),\n",
    "  #layers.RandomRotation(0.1),\n",
    "#])\n",
    "\n",
    "resize_and_rescale = tf.keras.Sequential([\n",
    "  layers.Resizing(180, 180),\n",
    "  layers.Rescaling(1./255)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c5a23c-9021-4201-93a8-777d69d4707f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "for images, _ in train_ds.take(22):\n",
    "    print(_)\n",
    "    for i in range(5):\n",
    "     augmented_images = data_augmentation(images)\n",
    "     ax = plt.subplot(3, 3, i + 1)\n",
    "     plt.imshow(augmented_images[0].numpy().astype(\"uint8\"))\n",
    "     plt.axis(\"off\")\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce8451e-cc5d-4a87-a66e-5d9f811bc284",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "  layers.Rescaling(1./255, input_shape=(img_height, img_width, 3)),\n",
    "  data_augmentation,  \n",
    "  layers.Conv2D(16, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Dropout(0.2),\n",
    "  layers.Flatten(),\n",
    "  layers.Dense(128, activation='relu'),\n",
    "  layers.Dense(5)\n",
    "])\n",
    "\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94dd195-ddbf-41bc-8b03-a0253e785564",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history=model.summary()\n",
    "\n",
    "epochs=20\n",
    "history = model.fit(\n",
    "  train_ds,\n",
    "  validation_data=val_ds,\n",
    "  epochs=epochs\n",
    ")\n",
    "\n",
    "history_df=pd.DataFrame(history.history)\n",
    "history_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18969dd-c5dd-40f1-ad32-0fa6e33f75a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=[0,1,2,3,4]\n",
    "classification_report1,confusion_matrix1,multi_confusion_matrix1=model_predict(model,train_ds, y_train,labels)\n",
    "classification_report_df, dict_matrix, confusion_matrix_df,aaa=model_summary(classification_report1,multi_confusion_matrix1,confusion_matrix1,labels)\n",
    "plot_model_results(history_df,epochs)\n",
    "\n",
    "\n",
    "print(classification_report_df)\n",
    "print(confusion_matrix_df)\n",
    "print(dict_matrix['Yellow Soil'])\n",
    "aaa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd52437f-44fe-46cb-b46f-107c72ee8d99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
